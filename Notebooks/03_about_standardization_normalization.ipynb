{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Machine Learning in scikit-learn\n",
    "*From the [Sebastian Raschka](https://github.com/rasbt/pattern_classification) the GitHub repository [pattern_classification](https://github.com/rasbt/pattern_classification)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Machine learning](images/01_robot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. About Feature Scaling: Z-Score Standardization and Min-Max Scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info about [Wine](http://archive.ics.uci.edu/ml/datasets/Wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Loading the wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv', \n",
    "     header=None,\n",
    "     usecols=[0,1,2]\n",
    "    )\n",
    "\n",
    "df.columns=['Class label', 'Alcohol', 'Malic acid']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('Different types of wines: ')\n",
    "print df['Class label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the table above, the features **Alcohol** (percent/volumen) and **Malic acid** (g/l) are measured on different scales, so that ***Feature Scaling*** is necessary important prior to any comparison or combination of these data.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Standardization and Min-Max scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for many machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. About standardization: *Z-score normalization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mu = 0$ and $\\sigma = 1$\n",
    "\n",
    "where $\\mu$ is the mean (average) and $\\sigma$ is the standard deviation from the mean; standard scores (also called ***z*** scores) of the samples are calculated as follows:\n",
    "\n",
    "\\begin{equation} z = \\frac{x - \\mu}{\\sigma}\\end{equation} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "std_scale = preprocessing.StandardScaler().fit(df[['Alcohol', 'Malic acid']])\n",
    "df_std = std_scale.transform(df[['Alcohol', 'Malic acid']])\n",
    "\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(df[['Alcohol', 'Malic acid']])\n",
    "df_minmax = minmax_scale.transform(df[['Alcohol', 'Malic acid']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Mean after standardization:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n",
    "      .format(df_std[:,0].mean(), df_std[:,1].mean()))\n",
    "print('\\nStandard deviation after standardization:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n",
    "      .format(df_std[:,0].std(), df_std[:,1].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. About normalization: *Min-Max scaling*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is scaled to a fixed range - usually 0 to 1.  \n",
    "The cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.\n",
    "\n",
    "\\begin{equation} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} \\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Min-value after min-max scaling:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n",
    "      .format(df_minmax[:,0].min(), df_minmax[:,1].min()))\n",
    "print('\\nMax-value after min-max scaling:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n",
    "      .format(df_minmax[:,0].max(), df_minmax[:,1].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Original input vs Standarization vs Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot():\n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    plt.scatter(df['Alcohol'], df['Malic acid'], \n",
    "            color='green', label='input scale', alpha=0.5)\n",
    "\n",
    "    plt.scatter(df_std[:,0], df_std[:,1], color='red', \n",
    "            label='Standardized [$N  (\\mu=0, \\; \\sigma=1)$]', alpha=0.3)\n",
    "\n",
    "    plt.scatter(df_minmax[:,0], df_minmax[:,1], \n",
    "            color='blue', label='min-max scaled [min=0, max=1]', alpha=0.3)\n",
    "\n",
    "    plt.title('Alcohol and Malic Acid content of the wine dataset')\n",
    "    plt.xlabel('Alcohol')\n",
    "    plt.ylabel('Malic Acid')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will zoom in into the three different axis-scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, figsize=(6,14))\n",
    "\n",
    "for a,d,l in zip(range(len(ax)), \n",
    "               (df[['Alcohol', 'Malic acid']].values, df_std, df_minmax),\n",
    "               ('Input scale', \n",
    "                'Standardized [$N  (\\mu=0, \\; \\sigma=1)$]', \n",
    "                'min-max scaled [min=0, max=1]')\n",
    "                ):\n",
    "    for i,c in zip(range(1,4), ('red', 'blue', 'green')):\n",
    "        ax[a].scatter(d[df['Class label'].values == i, 0], \n",
    "                  d[df['Class label'].values == i, 1],\n",
    "                  alpha=0.5,\n",
    "                  color=c,\n",
    "                  label='Class %s' %i\n",
    "                  )\n",
    "    ax[a].set_title(l)\n",
    "    ax[a].set_xlabel('Alcohol')\n",
    "    ax[a].set_ylabel('Malic Acid')\n",
    "    ax[a].legend(loc='upper left')\n",
    "    ax[a].grid()\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. The effect of standardization on PCA (Principal Components Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Loading the wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv', header=None,)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Split the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_wine = df.values[:,1:]\n",
    "y_wine = df.values[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine, test_size=0.3, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Dimensionality reduction via Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the dataset onto a 2-dimensional feature subspace.  \n",
    "In a real application, a procedure like cross-validation would be done ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# on non-standardized data\n",
    "pca = PCA(n_components=2).fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(X_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PCA - Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# on standardized data\n",
    "std_scale = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_std = std_scale.transform(X_train)\n",
    "X_test_std = std_scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on standardized data\n",
    "pca_std = PCA(n_components=2).fit(X_train_std)\n",
    "X_train_std = pca_std.transform(X_train_std)\n",
    "X_test_std = pca_std.transform(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(X_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_std[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Visualization\n",
    "Let us quickly visualize how our new feature subspace looks like (note that class labels are not considered in a PCA - in contrast to a Linear Discriminant Analysis - but I will add them in the plot for clarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "\n",
    "for l,c,m in zip(range(1,4), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "    ax1.scatter(X_train[y_train==l, 0], X_train[y_train==l, 1],\n",
    "        color=c, \n",
    "        label='class %s' %l, \n",
    "        alpha=0.5,\n",
    "        marker=m\n",
    "        )\n",
    "\n",
    "for l,c,m in zip(range(1,4), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "    ax2.scatter(X_train_std[y_train==l, 0], X_train_std[y_train==l, 1],\n",
    "        color=c, \n",
    "        label='class %s' %l, \n",
    "        alpha=0.5,\n",
    "        marker=m\n",
    "        )\n",
    "\n",
    "ax1.set_title('Transformed NON-standardized training dataset after PCA')    \n",
    "ax2.set_title('Transformed standardized training dataset after PCA')    \n",
    "    \n",
    "for ax in (ax1, ax2):\n",
    "\n",
    "    ax.set_xlabel('1st principal component')\n",
    "    ax.set_ylabel('2nd principal component')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Training a naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a naive Bayes classifier for the classification task. The term \"naive\" comes from the assumption that all features are \"independent\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# on non-standardized data\n",
    "gnb = GaussianNB()\n",
    "fit = gnb.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on standardized data\n",
    "gnb_std = GaussianNB()\n",
    "fit_std = gnb_std.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Evaluating the classification accuracy with and without standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "pred_test = gnb.predict(X_test_pca)\n",
    "\n",
    "print('\\nPrediction accuracy for the test dataset')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_test_std = gnb_std.predict(X_test_std)\n",
    "\n",
    "print('\\nPrediction accuracy for the test dataset')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
